name: Performance Regression Check

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark:
    name: Performance Regression Detection
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake build-essential

    - name: Configure CMake (Release)
      run: |
        cmake -B build -DCMAKE_BUILD_TYPE=Release

    - name: Build benchmark
      run: |
        cmake --build build --target simdcsv_benchmark --config Release

    - name: Run quick benchmark
      timeout-minutes: 5
      run: |
        # Run only specific benchmarks with limited iterations for speed
        # Filter to run essential benchmarks only, with JSON output for comparison
        # Benchmark names defined in benchmark/basic_benchmarks.cpp
        cd build
        ./simdcsv_benchmark \
          --benchmark_filter="BM_ParseSimple_Threads/1|BM_ParseManyRows_Threads/1|BM_ParseQuoted/1" \
          --benchmark_repetitions=3 \
          --benchmark_min_time=0.1s \
          --benchmark_out=benchmark_results.json \
          --benchmark_out_format=json

    - name: Download previous benchmark results
      uses: actions/cache/restore@v4
      id: cache-benchmark
      with:
        path: baseline_benchmark.json
        key: benchmark-baseline-${{ runner.os }}-main
        restore-keys: |
          benchmark-baseline-${{ runner.os }}-main

    - name: Compare benchmarks and detect regression
      id: compare
      run: |
        python3 << 'EOF'
import json
import sys
import os
import shutil

REGRESSION_THRESHOLD = 0.10  # 10% regression threshold
EXPECTED_BENCHMARK_COUNT = 3  # Number of benchmarks we expect to run

def load_benchmark(filepath):
    """Load benchmark results from JSON file."""
    if not os.path.exists(filepath):
        return None
    try:
        with open(filepath, 'r') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        print(f"ERROR: Failed to parse JSON from {filepath}: {e}")
        return None

    # Extract benchmark results, keyed by name
    results = {}
    for bench in data.get('benchmarks', []):
        name = bench.get('name', '')
        # Skip aggregate results (mean, median, stddev)
        if bench.get('aggregate_name'):
            continue
        # Use real_time as the primary metric (nanoseconds)
        if 'real_time' in bench:
            if name not in results:
                results[name] = []
            results[name].append(bench['real_time'])

    # Average multiple runs
    return {name: sum(times) / len(times) for name, times in results.items()}

def compare_benchmarks(baseline, current):
    """Compare current results against baseline and detect regressions."""
    regressions = []
    improvements = []
    unchanged = []

    for name, curr_time in current.items():
        if name not in baseline:
            unchanged.append((name, curr_time, None, None))
            continue

        base_time = baseline[name]
        # Positive ratio means regression (slower), negative means improvement
        ratio = (curr_time - base_time) / base_time

        if ratio > REGRESSION_THRESHOLD:
            regressions.append((name, base_time, curr_time, ratio))
        elif ratio < -REGRESSION_THRESHOLD:
            improvements.append((name, base_time, curr_time, ratio))
        else:
            unchanged.append((name, base_time, curr_time, ratio))

    return regressions, improvements, unchanged

# Check if this is a main branch push
is_main_push = os.environ.get('GITHUB_EVENT_NAME') == 'push' and \
               os.environ.get('GITHUB_REF') in ('refs/heads/main', 'refs/heads/master')

# Load results
current = load_benchmark('build/benchmark_results.json')
baseline = load_benchmark('baseline_benchmark.json')

if current is None:
    print("ERROR: Current benchmark results not found or invalid!")
    sys.exit(1)

# Validate we have the expected number of benchmarks
if len(current) < EXPECTED_BENCHMARK_COUNT:
    print(f"ERROR: Expected at least {EXPECTED_BENCHMARK_COUNT} benchmarks but got {len(current)}")
    print(f"Found benchmarks: {list(current.keys())}")
    sys.exit(1)

print("=" * 70)
print("PERFORMANCE REGRESSION CHECK")
print("=" * 70)
print(f"Regression threshold: {REGRESSION_THRESHOLD * 100:.0f}%")
print(f"Benchmarks found: {len(current)}")
print()

if baseline is None:
    print("No baseline found. This run will establish the baseline.")
    print()
    print("Current benchmark results:")
    for name, time in sorted(current.items()):
        print(f"  {name}: {time:.2f} ns")

    # Only save baseline on main branch pushes
    if is_main_push:
        shutil.copy('build/benchmark_results.json', 'baseline_benchmark.json')
        print()
        print("Baseline saved for future comparisons (main branch push).")
    else:
        print()
        print("Skipping baseline save (not a main branch push).")
else:
    regressions, improvements, unchanged = compare_benchmarks(baseline, current)

    if regressions:
        print("REGRESSIONS DETECTED:")
        for name, base, curr, ratio in regressions:
            print(f"  {name}")
            print(f"    Baseline: {base:.2f} ns")
            print(f"    Current:  {curr:.2f} ns")
            print(f"    Change:   +{ratio * 100:.1f}% (REGRESSION)")
        print()

    if improvements:
        print("IMPROVEMENTS:")
        for name, base, curr, ratio in improvements:
            print(f"  {name}")
            print(f"    Baseline: {base:.2f} ns")
            print(f"    Current:  {curr:.2f} ns")
            print(f"    Change:   {ratio * 100:.1f}%")
        print()

    if unchanged:
        print("UNCHANGED (within threshold):")
        for name, base, curr, ratio in unchanged:
            if ratio is not None:
                print(f"  {name}: {ratio * 100:+.1f}%")
            else:
                print(f"  {name}: (new benchmark)")
        print()

    if not regressions:
        if is_main_push:
            shutil.copy('build/benchmark_results.json', 'baseline_benchmark.json')
            print("Baseline updated with current results (main branch push).")
        print()
        print("SUCCESS: No performance regressions detected!")
    else:
        print()
        print(f"FAILURE: {len(regressions)} benchmark(s) regressed by more than {REGRESSION_THRESHOLD * 100:.0f}%")
        # Set output for GitHub Actions
        with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
            f.write('regression=true\n')
        sys.exit(1)

print("=" * 70)
EOF

    - name: Save baseline for future runs
      uses: actions/cache/save@v4
      if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
      with:
        path: baseline_benchmark.json
        key: benchmark-baseline-${{ runner.os }}-main

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: |
          build/benchmark_results.json
          baseline_benchmark.json
        retention-days: 30
